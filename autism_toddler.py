# -*- coding: utf-8 -*-
"""autism_toddler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xS_TeTf3F03Wz6HOEbYkUxoYdFSV1BBI
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

#LOADING DATA
df = pd.read_csv("Toddler_dataset.csv")

df1 = df.copy()

df.head()

df.shape

# check the data info
df.info()

#EXPLORATORY DATA ANALYSIS
df = df.rename(columns={"Age_Mons":"Age Months",
                        "Family_mem_with_ASD":"Family Member with ASD",
                        "Class/ASD Traits ": "ASD Traits"})
df.columns

df.isnull().mean() * 100

df["ASD Traits"].value_counts()

df["Ethnicity"] = df["Ethnicity"].replace("mixed", "Others")
df["Ethnicity"] = df["Ethnicity"].replace("Native Indian", "South Asian")
df["Ethnicity"] = df["Ethnicity"].replace("asian", "Other Asians")
df["Ethnicity"] = df["Ethnicity"].replace("middle eastern", "Middle Eastern")
df["Ethnicity"] = df["Ethnicity"].replace("south asian", "South Asian")
df["Ethnicity"] = df["Ethnicity"].replace("black", "African")

df["Ethnicity"].value_counts()

df["Who completed the test"].unique()

df["Who completed the test"] = df["Who completed the test"].replace("family member", "Family Member")
df["Who completed the test"] = df["Who completed the test"].replace("Health care professional", "Health Care Professional")

df["Who completed the test"].value_counts()

cat_df = df[["Sex", "Ethnicity", "Jaundice", "Who completed the test", "Family Member with ASD", "ASD Traits"]]
num_df = df[["A1", "A2", "A3", "A4","A5","A6", "A7", "A8", "A9", "A10", "Qchat-10-Score", "Age Months"]]

df.iloc[:, 13:].head(1)

for col in df.iloc[:, 13: ]:
    print(col,"\n", df[col].unique(), "\n")

df.describe()

#DATA VISUALIZATION
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame

# Select only numeric columns
numeric_df = df.select_dtypes(include=[float, int])

# Compute the correlation matrix
correlation_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

#What Features from A1 to A10 contributes the most in ASD tarits?
df.iloc[:, 1:11].head(1)

ax = df.iloc[:, 1:11].sum().sort_values().plot(kind="bar")
ax.bar_label(ax.containers[0]);

#ASD Traits
df["ASD Traits"].value_counts().plot(kind="pie", autopct="%1.1f%%", wedgeprops=dict(width=.3, edgecolor='w'));

#DATA PREPROCESSING
df1.drop(columns=["Case_No"], inplace=True)

mini = df1["Age_Mons"].min()
maxi = df1["Age_Mons"].max()
print(f"Minimum age {mini} and Maximum age {maxi}")

df1["Age"] = df1["Age_Mons"] / 12
df1.drop(columns=["Age_Mons"], inplace=True)

df1.head()

order = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',  'Age',
       'Qchat-10-Score', 'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD',
       'Who completed the test', 'Class/ASD Traits ']
df1 = df1[order]

df1.head()

df1.columns

df1["Class/ASD Traits "].unique()

df1["Class/ASD Traits "].value_counts()

#LABEL ENCODING
le = LabelEncoder()
df1["Sex"] = le.fit_transform(df1["Sex"])
df1["Jaundice"] = le.fit_transform(df1["Jaundice"])
df1["Family_mem_with_ASD"] = le.fit_transform(df1["Family_mem_with_ASD"])
df1["Class/ASD Traits "] = le.fit_transform(df1["Class/ASD Traits "])
df1.head()

#ONEHOT ENCODING
print(df1.columns)
df1 = pd.get_dummies(df1, columns=["Ethnicity", "Who completed the test"], drop_first=True)
df1.head()

df1.columns
order = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age',
       'Qchat-10-Score', 'Sex', 'Jaundice', 'Family_mem_with_ASD', 'Ethnicity_Latino',
        'Ethnicity_Native Indian','Ethnicity_Others', 'Ethnicity_Pacifica',
        'Ethnicity_White European','Ethnicity_asian', 'Ethnicity_black', 'Ethnicity_middle eastern',
       'Ethnicity_mixed', 'Ethnicity_south asian',
       'Who completed the test_Health care professional',
       'Who completed the test_Others', 'Who completed the test_Self',
       'Who completed the test_family member', 'Class/ASD Traits ']

df1 = df1[order]
df1.head()

#DATA SPLITTING
X = df1.drop(columns=["Class/ASD Traits "])
y = df1["Class/ASD Traits "]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#FEATURE SCALING
sc = MinMaxScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# Feature Scaling
sc = MinMaxScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# Plot the distribution of the target variable before splitting
plt.figure(figsize=(10, 5))
sns.countplot(x=y)
plt.title('Distribution of Target Variable Before Splitting')
plt.xlabel('Class/ASD Traits')
plt.ylabel('Count')
plt.show()

# Plot the feature values before scaling (using the first feature as an example)
plt.figure(figsize=(14, 7))

plt.subplot(1, 2, 1)
plt.hist(X_train.iloc[:, 0], bins=30, alpha=0.5, label='Train')
plt.hist(X_test.iloc[:, 0], bins=30, alpha=0.5, label='Test')
plt.title('Feature Values Before Scaling')
plt.xlabel('Feature 1')
plt.ylabel('Frequency')
plt.legend()

# Plot the feature values after scaling (using the first feature as an example)
plt.subplot(1, 2, 2)
plt.hist(X_train_scaled[:, 0], bins=30, alpha=0.5, label='Train')
plt.hist(X_test_scaled[:, 0], bins=30, alpha=0.5, label='Test')
plt.title('Feature Values After Scaling')
plt.xlabel('Scaled Feature 1')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

#MODEL BUILDING
def train_model(model, X_train_scaled, y_train, X_test_scaled, y_test):

    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return pd.DataFrame([[accuracy, precision, recall, f1]],
                            columns=["accuracy", "precision", "recall", "f1"])

#LOGISTIC Regression

model = LogisticRegression()

results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

results.index = ["Logistic Regression"]
results

#DECISION TREE CLASSIFIER

model = DecisionTreeClassifier()
decision_tree_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)
decision_tree_results.index = ["Decision Tree Classifier"]

results = pd.concat([results, decision_tree_results])
results

#ENSEMBLE TECHNIQUES

# RandomForestClassifier

model = RandomForestClassifier()
random_forest_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

random_forest_results.index = ["Random Forest Classifier"]

results = pd.concat([results, random_forest_results])
results

# GradientBoostingClassifier

model = GradientBoostingClassifier()
gradiet_boosting_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

gradiet_boosting_results.index = ["Gradient Boosting Classifier"]

results = pd.concat([results, gradiet_boosting_results])
results

# AdaBoostClassifier

model = AdaBoostClassifier()
ada_boost_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

ada_boost_results.index = ["Ada Boost Classifier"]

results = pd.concat([results, ada_boost_results])
results

#SUPPORT VECTOR MACHINE

model = SVC()
svm_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

svm_results.index = ["SVM Classifier"]

results = pd.concat([results, svm_results])
results

#K-NEIGHBORS

model = KNeighborsClassifier()
knn_results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

knn_results.index = ["KNeighbors Classifier"]

results = pd.concat([results, knn_results])
results

results = results.T
results.shape

results

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Function to train and evaluate models
def train_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    train_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    return pd.DataFrame({
        'Train Accuracy': [train_accuracy],
        'Test Accuracy': [test_accuracy]
    })

# ANN Model Training and Evaluation

# ANN Model
ann_model = Sequential()
ann_model.add(Dense(32, activation="relu", input_dim=X_train_scaled.shape[1]))
ann_model.add(Dense(16, activation="relu"))
ann_model.add(Dense(1, activation="sigmoid"))

ann_model.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["accuracy"])

ann_callback = EarlyStopping(monitor="val_loss", min_delta=0.001, patience=10, verbose=1, mode="auto", restore_best_weights=True)

ann_history = ann_model.fit(X_train_scaled, y_train, batch_size=10, epochs=12, validation_split=0.2, callbacks=[ann_callback])

# Evaluate the ANN model
ann_train_accuracy = ann_model.evaluate(X_train_scaled, y_train, verbose=0)[1]
ann_test_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]

ann_results = pd.DataFrame({
    'Train Accuracy': [ann_train_accuracy],
    'Test Accuracy': [ann_test_accuracy]
})

ann_results.index = ["ANN Classifier"]
results = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)
results = pd.concat([results, ann_results])
print(ann_results)


# Plotting training history for ANN
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Plotting accuracy
plt.subplot(1, 2, 1)
plt.plot(ann_history.history['accuracy'], label='Train Accuracy')
plt.plot(ann_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('ANN Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plotting loss
plt.subplot(1, 2, 2)
plt.plot(ann_history.history['loss'], label='Train Loss')
plt.plot(ann_history.history['val_loss'], label='Validation Loss')
plt.title('ANN Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score

# Function to train and evaluate models
def train_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    precision = precision_score(y_test, y_pred_test)
    recall = recall_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)

    cm = confusion_matrix(y_test, y_pred_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else model.decision_function(X_test)
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)

    return pd.DataFrame({
        'Train Accuracy': [train_accuracy],
        'Test Accuracy': [test_accuracy],
        'Precision': [precision],
        'Recall': [recall],
        'F1 Score': [f1],
        'AUC': [roc_auc]
    }), cm, (fpr, tpr, roc_auc)

# Assume X_train_scaled, y_train, X_test_scaled, y_test are your preprocessed datasets

# Initialize result containers
results = pd.DataFrame()
confusion_matrices = {}
roc_curves = {}

# Define models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree Classifier": DecisionTreeClassifier(),
    "Random Forest Classifier": RandomForestClassifier(),
    "Gradient Boosting Classifier": GradientBoostingClassifier(),
    "Ada Boost Classifier": AdaBoostClassifier(),
    "SVM Classifier": SVC(probability=True),
    "KNeighbors Classifier": KNeighborsClassifier()
}

# Train and evaluate each model
for model_name, model in models.items():
    model_results, cm, roc_curve_data = train_model(model, X_train_scaled, y_train, X_test_scaled, y_test)
    model_results.index = [model_name]
    results = pd.concat([results, model_results])
    confusion_matrices[model_name] = cm
    roc_curves[model_name] = roc_curve_data

# ANN Model Training and Evaluation
ann_model = Sequential()
ann_model.add(Dense(32, activation="relu", input_dim=X_train_scaled.shape[1]))
ann_model.add(Dense(16, activation="relu"))
ann_model.add(Dense(1, activation="sigmoid"))

ann_model.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["accuracy"])

ann_callback = EarlyStopping(monitor="val_loss", min_delta=0.001, patience=10, verbose=1, mode="auto", restore_best_weights=True)

ann_history = ann_model.fit(X_train_scaled, y_train, batch_size=10, epochs=12, validation_split=0.2, callbacks=[ann_callback])

# Evaluate the ANN model
ann_train_accuracy = ann_model.evaluate(X_train_scaled, y_train, verbose=0)[1]
ann_test_accuracy = ann_model.evaluate(X_test_scaled, y_test, verbose=0)[1]
ann_precision = precision_score(y_test, (ann_model.predict(X_test_scaled) > 0.5).astype("int32"))
ann_recall = recall_score(y_test, (ann_model.predict(X_test_scaled) > 0.5).astype("int32"))
ann_f1 = f1_score(y_test, (ann_model.predict(X_test_scaled) > 0.5).astype("int32"))

ann_y_pred_prob = ann_model.predict(X_test_scaled).ravel()
ann_fpr, ann_tpr, _ = roc_curve(y_test, ann_y_pred_prob)
ann_roc_auc = auc(ann_fpr, ann_tpr)

ann_results = pd.DataFrame({
    'Train Accuracy': [ann_train_accuracy],
    'Test Accuracy': [ann_test_accuracy],
    'Precision': [ann_precision],
    'Recall': [ann_recall],
    'F1 Score': [ann_f1],
    'AUC': [ann_roc_auc]
})

ann_results.index = ["ANN Classifier"]
results = pd.concat([results, ann_results])

# Generate confusion matrix for ANN
ann_cm = confusion_matrix(y_test, (ann_model.predict(X_test_scaled) > 0.5).astype("int32"))
confusion_matrices['ANN Classifier'] = ann_cm
roc_curves['ANN Classifier'] = (ann_fpr, ann_tpr, ann_roc_auc)

# Display results
print(results)

# Function to plot confusion matrices
def plot_confusion_matrices(confusion_matrices):
    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))
    axes = axes.flatten()
    for i, (model_name, cm) in enumerate(confusion_matrices.items()):
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[i])
        axes[i].set_title(f'Confusion Matrix for {model_name}')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')
    plt.tight_layout()
    plt.show()

plot_confusion_matrices(confusion_matrices)

# Function to plot ROC curves
def plot_roc_curves(roc_curves):
    plt.figure(figsize=(10, 8))
    for model_name, (fpr, tpr, roc_auc) in roc_curves.items():
        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curves')
    plt.legend(loc="lower right")
    plt.show()

plot_roc_curves(roc_curves)